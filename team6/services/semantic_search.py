# from langchain_community.embeddings import HuggingFaceEmbeddings
# from langchain_community.vectorstores import FAISS
# from deep_translator import GoogleTranslator


# def simple_normalize(text: str) -> str:
#     return (
#         text.replace("ÙŠ", "ÛŒ")
#             .replace("Ùƒ", "Ú©")
#             .replace("â€Œ", " ")
#             .strip()
#     )


# class SemanticSearchService:
#     def __init__(self):
#         self.embeddings = HuggingFaceEmbeddings(
#             model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
#         )

#     def search(self, articles, query, k=10):
#         """
#         articles: list[WikiArticle]
#         query: str
#         """

#         if not articles:
#             return []
#         translated_query = GoogleTranslator(source='fa', target='en').translate(query)
#         documents = [
#             simple_normalize(
#                 f"{GoogleTranslator(source='fa', target='en').translate(a.title_fa) or ''} {GoogleTranslator(source='fa', target='en').translate(a.summary) or ''} {GoogleTranslator(source='fa', target='en').translate(a.body_fa) or ''}"
#             )
#             for a in articles
#         ]

#         vectorstore = FAISS.from_texts(documents, self.embeddings)

#         results = vectorstore.similarity_search_with_score(translated_query, k=k)

#         # Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù† Ù…Ù‚Ø§Ù„Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ ØªØ±ØªÛŒØ¨ Ø´Ø¨Ø§Ù‡Øª Ù…Ø¹Ù†Ø§ÛŒÛŒ
#         ranked_articles = []
#         for doc, score in results:
#             index = documents.index(doc.page_content)
#             ranked_articles.append((articles[index], score))

#         return ranked_articles


import os
import logging
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from deep_translator import GoogleTranslator

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù„Ø§Ú¯Ø± Ø¨Ø±Ø§ÛŒ Ù…Ø´Ø§Ù‡Ø¯Ù‡ ÙˆØ¶Ø¹ÛŒØª Ø¯Ø± ØªØ±Ù…ÛŒÙ†Ø§Ù„
logger = logging.getLogger(__name__)

def simple_normalize(text: str) -> str:
    """Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù…Ù‚Ø¯Ù…Ø§ØªÛŒ Ù…ØªÙ† Ø¨Ø±Ø§ÛŒ Ø­Ø°Ù ØªØ¯Ø§Ø®Ù„ Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ Ø¹Ø±Ø¨ÛŒ Ùˆ ÙØ§Ø±Ø³ÛŒ"""
    if not text:
        return ""
    return (
        text.replace("ÙŠ", "ÛŒ")
            .replace("Ùƒ", "Ú©")
            .replace("â€Œ", " ")
            .strip()
    )

class SemanticSearchService:
    def __init__(self):
        # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ù…Ø³ÛŒØ± Ù¾ÙˆØ´Ù‡ Ø§Ù¾Ù„ÛŒÚ©ÛŒØ´Ù† team6 Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø§ÛŒÙ†Ø¯Ú©Ø³
        # __file__ Ø¢Ø¯Ø±Ø³ ÙØ§ÛŒÙ„ ÙØ¹Ù„ÛŒ Ø§Ø³ØªØŒ Ù¾Ø³ dirname Ø¢Ù† Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ù¾ÙˆØ´Ù‡ services Ùˆ dirname Ø¨Ø¹Ø¯ÛŒ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ù¾ÙˆØ´Ù‡ team6
        current_dir = os.path.dirname(os.path.abspath(__file__))
        self.index_path = os.path.join(os.path.dirname(current_dir), "faiss_index")
        
        # Ù„ÙˆØ¯ Ù…Ø¯Ù„ Ø§Ù…Ø¨Ø¯ÛŒÙ†Ú¯ (Ø§ÛŒÙ† Ù…Ø¯Ù„ Ú†Ù†Ø¯Ø²Ø¨Ø§Ù†Ù‡ Ø§Ø³Øª Ùˆ Ø¯Ø± Ø±Ù… Ù„ÙˆØ¯ Ù…ÛŒâ€ŒØ´ÙˆØ¯)
        logger.info("Loading HuggingFace Embeddings...")
        self.embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
        )
        self.vectorstore = self._load_index()

    def _load_index(self):
        """ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø§ÛŒÙ†Ø¯Ú©Ø³ Ø§Ø² ÙØ§ÛŒÙ„ Ù…Ø­Ù„ÛŒ Ø¯Ø± Ù¾ÙˆØ´Ù‡ team6"""
        if os.path.exists(self.index_path):
            try:
                logger.info(f"Loading existing FAISS index from {self.index_path}")
                return FAISS.load_local(
                    self.index_path, 
                    self.embeddings, 
                    allow_dangerous_deserialization=True
                )
            except Exception as e:
                logger.error(f"Error loading index: {e}")
        return None

    def build_index(self, articles):
        """ØªØ±Ø¬Ù…Ù‡ Ù…Ù‚Ø§Ù„Ø§Øª Ùˆ Ø³Ø§Ø®Øª Ø§ÛŒÙ†Ø¯Ú©Ø³ Ø¨Ø±Ø§ÛŒ Ø§ÙˆÙ„ÛŒÙ† Ø¨Ø§Ø±"""
        if not articles:
            return None
        
        logger.info("ğŸš€ Building semantic index. This may take a minute (translating articles)...")
        
        documents = []
        metadatas = []
        translator = GoogleTranslator(source='fa', target='en')

        for a in articles:
            try:
                # ØªØ±Ú©ÛŒØ¨ ØªØ§ÛŒØªÙ„ØŒ Ø®Ù„Ø§ØµÙ‡ Ùˆ Ø¨Ø®Ø´ÛŒ Ø§Ø² Ù…ØªÙ† Ø¨Ø±Ø§ÛŒ ØªØ±Ø¬Ù…Ù‡ Ùˆ Ø§ÛŒÙ†Ø¯Ú©Ø³
                full_text_fa = f"{a.title_fa} {a.summary or ''} {a.body_fa[:400]}"
                # ØªØ±Ø¬Ù…Ù‡ Ø¨Ù‡ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ù„Ø§ Ø±ÙØªÙ† Ø¯Ù‚Øª Ù…Ø¯Ù„ Ú†Ù†Ø¯Ø²Ø¨Ø§Ù†Ù‡
                translated_text = translator.translate(full_text_fa)
                
                documents.append(simple_normalize(translated_text))
                metadatas.append({"id": str(a.id)}) 
            except Exception as e:
                logger.warning(f"Skipping article {a.id} due to error: {e}")
                continue

        if not documents:
            return None

        # Ø³Ø§Ø®Øª Ø§ÛŒÙ†Ø¯Ú©Ø³ FAISS
        vectorstore = FAISS.from_texts(documents, self.embeddings, metadatas=metadatas)
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ù¾ÙˆØ´Ù‡ team6/faiss_index
        vectorstore.save_local(self.index_path)
        self.vectorstore = vectorstore
        logger.info(f"âœ… Index built and saved to {self.index_path}")
        return vectorstore

    def search(self, articles, query, k=10):
        """
        Ø¬Ø³ØªØ¬ÙˆÛŒ Ù…Ø¹Ù†Ø§ÛŒÛŒ Ø¨ÛŒÙ† Ù…Ù‚Ø§Ù„Ø§Øª
        articles: Ú©ÙˆØ¦Ø±ÛŒâ€ŒØ³Øª Ù…Ù‚Ø§Ù„Ø§Øª Ø¬Ù†Ú¯Ùˆ
        query: Ù…ØªÙ† Ø¬Ø³ØªØ¬ÙˆÛŒ Ú©Ø§Ø±Ø¨Ø±
        k: ØªØ¹Ø¯Ø§Ø¯ Ù†ØªØ§ÛŒØ¬
        """
        # Ø§Ú¯Ø± Ø§ÛŒÙ†Ø¯Ú©Ø³ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯ØŒ Ù‡Ù…ÛŒÙ† Ø­Ø§Ù„Ø§ Ø¢Ù† Ø±Ø§ Ø¨Ø³Ø§Ø²
        if self.vectorstore is None:
            self.build_index(articles)
        
        if not self.vectorstore:
            return []

        # Û±. ØªØ±Ø¬Ù…Ù‡ Ú©ÙˆØ¦Ø±ÛŒ Ú©Ø§Ø±Ø¨Ø± Ø¨Ù‡ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ
        try:
            translated_query = GoogleTranslator(source='fa', target='en').translate(query)
        except Exception:
            translated_query = query

        # Û². Ø¬Ø³ØªØ¬ÙˆÛŒ Ø´Ø¨Ø§Ù‡Øª (Ø®Ø±ÙˆØ¬ÛŒ: Ù„ÛŒØ³Øª Ø¯Ø§Ú©ÛŒÙˆÙ…Ù†Øªâ€ŒÙ‡Ø§ Ùˆ Ø§Ù…ØªÛŒØ§Ø² ÙØ§ØµÙ„Ù‡)
        # Ù†Ú©ØªÙ‡: Ø¯Ø± FAISS Ù‡Ø±Ú†Ù‡ Score Ú©Ù…ØªØ± Ø¨Ø§Ø´Ø¯ (Ù†Ø²Ø¯ÛŒÚ© Ø¨Ù‡ ØµÙØ±)ØŒ Ø´Ø¨Ø§Ù‡Øª Ø¨ÛŒØ´ØªØ± Ø§Ø³Øª.
        results = self.vectorstore.similarity_search_with_score(translated_query, k=k)

        # Û³. Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¬Ù†Ú¯Ùˆ Ø¨Ø± Ø§Ø³Ø§Ø³ IDÙ‡Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡ Ø¯Ø± Ù…ØªØ§Ø¯ÛŒØªØ§
        ranked_articles = []
        article_dict = {str(a.id): a for a in articles}
        
        # Ø­Ø¯ Ø¢Ø³ØªØ§Ù†Ù‡ Ø¨Ø±Ø§ÛŒ Ø´Ø¨Ø§Ù‡Øª (Ù‚Ø§Ø¨Ù„ ØªÙ†Ø¸ÛŒÙ…: Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Ø¨ÛŒÙ† 0.4 ØªØ§ 0.8)
        # Ù‡Ø±Ú†Ù‡ Ø§ÛŒÙ† Ø¹Ø¯Ø¯ Ú©Ù…ØªØ± Ø¨Ø§Ø´Ø¯ØŒ Ø¬Ø³ØªØ¬Ùˆ Ø³Ø®Øªâ€ŒÚ¯ÛŒØ±Ø§Ù†Ù‡â€ŒØªØ± Ù…ÛŒâ€ŒØ´ÙˆØ¯.
        DISTANCE_THRESHOLD = 0.6 

        for doc, score in results:
            if score <= DISTANCE_THRESHOLD:
                article_id = doc.metadata.get("id")
                if article_id in article_dict:
                    ranked_articles.append((article_dict[article_id], score))

        return ranked_articles